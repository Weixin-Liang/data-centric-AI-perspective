{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Extract Feature for Shapley Calculation\n",
        "=============================\n",
        "\n",
        "**Adopted from:** [Official PyTorch tutorial on fine-tuning torchvision models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Version:  1.4.0\n",
            "Torchvision Version:  0.5.0\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function \n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import pickle\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_dir = '/data/GQA/Perspective-Merged'\n",
        "model_name = \"resnet\"\n",
        "num_classes = 2\n",
        "batch_size = 64\n",
        "num_epochs = 1\n",
        "feature_extract = True "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def extract_model_features(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val', 'shapley_val',]:\n",
        "            model.eval()   # Set model to evaluate mode, for extraction\n",
        "\n",
        "            ##################################\n",
        "            # Fields to be stored for postprocessing \n",
        "            ##################################\n",
        "            feature_all = []\n",
        "            target_all = []\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(False):\n",
        "                    outputs  = model(inputs)\n",
        "                    features = outputs\n",
        "                    # _, preds = torch.max(outputs, 1)\n",
        "                    with torch.no_grad():\n",
        "                        ##################################\n",
        "                        # Evaluation book-keeping Field \n",
        "                        ##################################\n",
        "                        feature_all.append( outputs.cpu().numpy() )\n",
        "                        target_all.append( labels.cpu().numpy() )\n",
        "\n",
        "            ##################################\n",
        "            # Evaluation book-keeping Field \n",
        "            ##################################\n",
        "            target_all     = np.concatenate( target_all, axis=0)\n",
        "            feature_all     = np.concatenate( feature_all, axis=0)\n",
        "            print('feature_all', feature_all.shape)\n",
        "\n",
        "            dump_result_dict = {\n",
        "                \"target_all\": target_all, \n",
        "                \"feature_all\": feature_all, \n",
        "                }\n",
        "            with open(os.path.join('outputs', 'feature_dump_{}.pkl'.format(phase) ), \"wb\") as pkl_file:\n",
        "                pickle.dump(\n",
        "                    dump_result_dict, \n",
        "                    pkl_file, \n",
        "                )\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Identity() # nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "    \n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "# print(model_ft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Datasets and Dataloaders...\n"
          ]
        }
      ],
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "extract_transform = transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), extract_transform) for x in ['train', 'val', 'shapley_val']}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {\n",
        "    'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=False, num_workers=4), \n",
        "    'val'  : torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=False, num_workers=4),\n",
        "    'shapley_val'  : torch.utils.data.DataLoader(image_datasets['shapley_val'], batch_size=batch_size, shuffle=False, num_workers=4),\n",
        "}\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "optimizer_ft = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run Training and Validation Step\n",
        "--------------------------------\n",
        "\n",
        "Finally, the last step is to setup the loss for the model, then run the\n",
        "training and validation function for the set number of epochs. Notice,\n",
        "depending on the number of epochs this step may take a while on a CPU.\n",
        "Also, the default learning rate is not optimal for all of the models, so\n",
        "to achieve maximum accuracy it would be necessary to tune for each model\n",
        "separately.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/0\n",
            "----------\n",
            "feature_all (1500, 512)\n",
            "feature_all (240, 512)\n",
            "feature_all (238, 512)\n",
            "\n",
            "Training complete in 0m 7s\n",
            "Best val Acc: 0.000000\n"
          ]
        }
      ],
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = extract_model_features(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 240\n",
              "    Root location: /data/GQA/Perspective-Merged/val\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=224, interpolation=PIL.Image.BILINEAR)\n",
              "               CenterCrop(size=(224, 224))\n",
              "               ToTensor()\n",
              "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "           )"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_datasets['val']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 238\n",
              "    Root location: /data/GQA/Perspective-Merged/shapley_val\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=224, interpolation=PIL.Image.BILINEAR)\n",
              "               CenterCrop(size=(224, 224))\n",
              "               ToTensor()\n",
              "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "           )"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_datasets['shapley_val']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
